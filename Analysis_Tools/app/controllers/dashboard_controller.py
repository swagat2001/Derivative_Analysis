import csv
import json
import os
import time
from datetime import datetime
from io import BytesIO

import pandas as pd
from flask import Blueprint, Response, jsonify, render_template, request, send_file, stream_with_context

from ..models.dashboard_model import get_available_dates, get_dashboard_data
from ..models.stock_model import get_available_dates

dashboard_bp = Blueprint("dashboard", __name__)

# Path to your live CSV data
SPOT_DATA_PATH = r"C:\Users\Admin\Desktop\Derivative_Analysis\spot_data\Data\SpotData.csv"


# --------------------------------------------------------------------
# üß† Helper: Read live spot prices
# --------------------------------------------------------------------
def get_live_indices():
    try:
        if not os.path.exists(SPOT_DATA_PATH):
            print(f"[WARN] SpotData.csv not found at {SPOT_DATA_PATH}")
            return {"NIFTY": 0.0, "BANKNIFTY": 0.0, "GOLD": 0.0, "SILVER": 0.0}

        df = pd.read_csv(SPOT_DATA_PATH)
        df.columns = [c.strip().upper() for c in df.columns]

        return {
            "NIFTY": float(df.iloc[0]["NIFTY"]),
            "BANKNIFTY": float(df.iloc[0]["BANK NIFTY"]),
            "GOLD": float(df.iloc[0]["GOLD"]),
            "SILVER": float(df.iloc[0]["SILVER"]),
        }

    except Exception as e:
        print(f"[ERROR] get_live_indices(): {e}")
        return {"NIFTY": 0.0, "BANKNIFTY": 0.0, "GOLD": 0.0, "SILVER": 0.0}


# --------------------------------------------------------------------
# üîç Helper: Get stock list for search dropdown
# --------------------------------------------------------------------
def get_stock_list():
    """Get filtered stock list for search dropdown in header"""
    try:
        from ..models.stock_model import get_filtered_tickers

        return get_filtered_tickers()
    except Exception as e:
        print(f"[ERROR] get_stock_list(): {e}")
        return []


# --------------------------------------------------------------------
# üìä Dashboard route
# --------------------------------------------------------------------
@dashboard_bp.route("/")
def dashboard():
    try:
        dates = get_available_dates()

        # Handle empty dates gracefully
        if not dates:
            return render_template(
                "dashboard.html",
                data=[],
                dates=[],
                selected_date=None,
                mtype="TOTAL",
                indices=get_live_indices(),
                stock_list=get_stock_list(),
                stock_symbol=None,
                page=1,
                per_page=20,
                total_records=0,
                total_pages=0,
                sort_column="stock",
                sort_direction="asc",
            )

        # Get filter parameters
        selected_date = request.args.get("date", dates[0])
        mtype = request.args.get("mtype", "TOTAL")

        # Get pagination parameters
        page = request.args.get("page", 1, type=int)
        per_page = request.args.get("per_page", 20, type=int)

        # Get sorting parameters
        sort_column = request.args.get("sort", "stock")
        sort_direction = request.args.get("dir", "asc")

        # Fetch all data (DON'T paginate for client-side DataTables)
        all_data = get_dashboard_data(selected_date, mtype)
        data_list = [dict(row) for row in all_data]

        return render_template(
            "dashboard.html",
            data=data_list,  # Send ALL data for client-side DataTables
            dates=dates,
            selected_date=selected_date,
            mtype=mtype,
            indices=get_live_indices(),
            stock_list=get_stock_list(),
            stock_symbol=None,
        )
    except Exception as e:
        print(f"[ERROR] Dashboard route failed: {e}")
        import traceback

        traceback.print_exc()
        return jsonify({"error": f"Dashboard rendering failed: {str(e)}"}), 500


# --------------------------------------------------------------------
# üìä Server-Side DataTables API Endpoint
# --------------------------------------------------------------------
@dashboard_bp.route("/api/dashboard_data")
def api_dashboard_data():
    """
    Server-side processing endpoint for DataTables.
    Handles pagination, sorting, and filtering on the server.
    """
    try:
        # Get DataTables parameters
        draw = request.args.get("draw", type=int, default=1)
        start = request.args.get("start", type=int, default=0)
        length = request.args.get("length", type=int, default=20)

        # Get filter parameters
        selected_date = request.args.get("date")
        mtype = request.args.get("mtype", "TOTAL")

        # Get sorting parameters
        order_column = request.args.get("order[0][column]", type=int, default=0)
        order_dir = request.args.get("order[0][dir]", default="asc")

        # Fetch all data from database
        all_data = get_dashboard_data(selected_date, mtype)
        total_records = len(all_data)

        # Apply sorting
        column_map = {
            0: "stock",
            1: "call_delta_pos_strike",
            2: "call_delta_pos_pct",
            3: "call_delta_neg_strike",
            4: "call_delta_neg_pct",
            5: "call_vega_pos_strike",
            6: "call_vega_pos_pct",
            7: "call_vega_neg_strike",
            8: "call_vega_neg_pct",
            9: "call_total_trad_val",
            10: "call_total_money",
            11: "closing_price",
            12: "rsi",
            13: "put_delta_pos_strike",
            14: "put_delta_pos_pct",
            15: "put_delta_neg_strike",
            16: "put_delta_neg_pct",
            17: "put_vega_pos_strike",
            18: "put_vega_pos_pct",
            19: "put_vega_neg_strike",
            20: "put_vega_neg_pct",
            21: "put_total_trad_val",
            22: "put_total_money",
        }

        sort_column = column_map.get(order_column, "stock")
        reverse = order_dir == "desc"

        # Convert to list of dicts for sorting
        data_list = [dict(row) for row in all_data]

        # Sort data
        try:
            data_list.sort(key=lambda x: x.get(sort_column) or "", reverse=reverse)
        except:
            pass  # If sorting fails, keep original order

        # Apply pagination
        paginated_data = data_list[start : start + length]

        # Format data for DataTables
        formatted_data = []
        for row in paginated_data:
            formatted_data.append(
                [
                    f'<a href="/stock/{row["stock"]}">{row["stock"]}</a>',
                    row.get("call_delta_pos_strike", ""),
                    row.get("call_delta_pos_pct", ""),
                    row.get("call_delta_neg_strike", ""),
                    row.get("call_delta_neg_pct", ""),
                    row.get("call_vega_pos_strike", ""),
                    row.get("call_vega_pos_pct", ""),
                    row.get("call_vega_neg_strike", ""),
                    row.get("call_vega_neg_pct", ""),
                    f'{row.get("call_total_trad_val", 0):.2f}',
                    f'{row.get("call_total_money", 0):.2f}',
                    f'{row.get("closing_price", 0):.2f}',
                    f'{row.get("rsi", 0):.2f}',
                    row.get("put_delta_pos_strike", ""),
                    row.get("put_delta_pos_pct", ""),
                    row.get("put_delta_neg_strike", ""),
                    row.get("put_delta_neg_pct", ""),
                    row.get("put_vega_pos_strike", ""),
                    row.get("put_vega_pos_pct", ""),
                    row.get("put_vega_neg_strike", ""),
                    row.get("put_vega_neg_pct", ""),
                    f'{row.get("put_total_trad_val", 0):.2f}',
                    f'{row.get("put_total_money", 0):.2f}',
                ]
            )

        # Return DataTables JSON response
        return jsonify(
            {
                "draw": draw,
                "recordsTotal": total_records,
                "recordsFiltered": total_records,
                "data": formatted_data,
            }
        )

    except Exception as e:
        print(f"[ERROR] API dashboard_data failed: {e}")
        import traceback

        traceback.print_exc()
        return (
            jsonify(
                {
                    "draw": request.args.get("draw", type=int, default=1),
                    "recordsTotal": 0,
                    "recordsFiltered": 0,
                    "data": [],
                    "error": str(e),
                }
            ),
            500,
        )


# --------------------------------------------------------------------
# üîÅ API endpoint (optional)
# --------------------------------------------------------------------
@dashboard_bp.route("/api/live_indices")
def api_live_indices():
    return jsonify(get_live_indices())


# -----------------------------------------------------------
# √∞≈∏‚Äú¬§ Export to Excel
# -----------------------------------------------------------
@dashboard_bp.route("/export", methods=["GET"])
def export_dashboard():
    try:
        date = request.args.get("date")
        mtype = request.args.get("mtype", "TOTAL")

        # Fetch data for export
        data = get_dashboard_data(date, mtype)
        df = pd.DataFrame(data)

        # Create Excel in memory
        output = BytesIO()

        with pd.ExcelWriter(output, engine="openpyxl") as writer:
            # Sheet 1: Raw Data
            df.to_excel(writer, index=False, sheet_name="Dashboard_Data")

            # Add a formatted header sheet
            workbook = writer.book
            summary_sheet = workbook.create_sheet(title="Summary")

            summary_sheet["A1"] = "Derivatives Analysis Dashboard Export"
            summary_sheet["A1"].font = summary_sheet["A1"].font.copy(bold=True, size=14)

            summary_sheet["A3"] = "Date"
            summary_sheet["B3"] = date
            summary_sheet["A4"] = "Mode"
            summary_sheet["B4"] = mtype
            summary_sheet["A6"] = "Indices Snapshot"
            summary_sheet["A6"].font = summary_sheet["A6"].font.copy(bold=True)

            # Write index values
            indices = get_live_indices()
            row = 7
            for key, val in indices.items():
                summary_sheet[f"A{row}"] = key
                summary_sheet[f"B{row}"] = val
                row += 1

        output.seek(0)

        filename = f"dashboard_export_{date}_{mtype}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"

        return send_file(
            output,
            as_attachment=True,
            download_name=filename,
            mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )

    except ModuleNotFoundError as e:
        return (
            jsonify({"error": "Missing dependency: openpyxl not installed. Please run `pip install openpyxl`."}),
            500,
        )

    except Exception as e:
        return jsonify({"error": f"Export failed: {str(e)}"}), 500


# --------------------------------------------------------------------
# üß© Server-Sent Events (live streaming)
# --------------------------------------------------------------------
@dashboard_bp.route("/stream/live_indices")
def stream_live_indices():
    """Continuously stream CSV changes as Server-Sent Events (SSE)."""

    def event_stream():
        last_values = None
        while True:
            try:
                if os.path.exists(SPOT_DATA_PATH):
                    with open(SPOT_DATA_PATH, "r") as f:
                        reader = csv.DictReader(f)
                        row = next(reader)
                        current = {
                            "NIFTY": float(row["Nifty"]),
                            "BANKNIFTY": float(row["Bank Nifty"]),
                            "GOLD": float(row["Gold"]),
                            "SILVER": float(row["Silver"]),
                        }

                    # Only push when data changes
                    if current != last_values:
                        yield f"data: {json.dumps(current)}\n\n"
                        last_values = current
            except Exception as e:
                yield f"event: error\ndata: {str(e)}\n\n"

            time.sleep(0.1)  # checks every 100ms (~10 updates/sec)

    return Response(stream_with_context(event_stream()), mimetype="text/event-stream")


@dashboard_bp.route("/api/historical-chart-data")
def get_historical_chart_data():
    """
    OPTIMIZED: Returns 40-day historical data using SINGLE batch query instead of 40+ queries
    This reduces query time from 8-10 seconds to under 1 second.
    """
    ticker = request.args.get("ticker")
    option_type = request.args.get("option_type")  # 'call' or 'put'
    metric = request.args.get("metric")
    strike = request.args.get("strike")
    curr_date = request.args.get("date")

    if not all([ticker, option_type, metric, curr_date]):
        return jsonify({"error": "Missing parameters"}), 400

    try:
        import json

        from sqlalchemy import text

        from ..models.db_config import engine

        table_name = f"TBL_{ticker}_DERIVED"
        base_table = f"TBL_{ticker}"
        opt_type_param = "CE" if option_type == "call" else "PE"

        # -------------------------------------------------------------
        # üöÄ OPTIMIZED: Single batch query for ALL 40 days of data
        # -------------------------------------------------------------
        batch_query = text(
            f"""
            WITH date_range AS (
                SELECT DISTINCT "BizDt"::DATE AS "BizDt"
                FROM "{table_name}"
                WHERE "BizDt"::DATE <= CAST(:curr_date AS DATE)
                ORDER BY "BizDt" DESC
                LIMIT 40
            ),
            derived_data AS (
                SELECT
                    d."BizDt"::DATE AS "BizDt",
                    d."OptnTp",
                    d."StrkPric",
                    d."TtlTradgVol",
                    d."OpnIntrst",
                    d."TtlTrfVal",
                    d."vega"
                FROM "{table_name}" d
                INNER JOIN date_range dr ON d."BizDt"::DATE = dr."BizDt"
            ),
            base_data AS (
                SELECT DISTINCT
                    b."BizDt"::DATE AS "BizDt",
                    b."UndrlygPric"
                FROM "{base_table}" b
                INNER JOIN date_range dr ON b."BizDt"::DATE = dr."BizDt"
            ),
            cache_data AS (
                SELECT
                    c.biz_date::DATE AS "BizDt",
                    c.data_json
                FROM options_dashboard_cache c
                INNER JOIN date_range dr ON c.biz_date::DATE = dr."BizDt"
                WHERE c.moneyness_type = 'TOTAL'
            )
            SELECT
                d."BizDt",
                d."OptnTp",
                d."StrkPric",
                d."TtlTradgVol",
                d."OpnIntrst",
                d."TtlTrfVal",
                d."vega",
                b."UndrlygPric",
                c.data_json
            FROM derived_data d
            LEFT JOIN base_data b ON d."BizDt" = b."BizDt"
            LEFT JOIN cache_data c ON d."BizDt" = c."BizDt"
            ORDER BY d."BizDt", d."OptnTp", d."StrkPric"
        """
        )

        # Execute single optimized query (replaces 40+ queries!)
        df_all = pd.read_sql(batch_query, engine, params={"curr_date": curr_date})

        if df_all.empty:
            return jsonify({"error": "No data", "success": False}), 404

        # Process data in memory (much faster than multiple DB queries)
        historical_data = []
        dates = sorted(df_all["BizDt"].unique())

        for date in dates:
            try:
                date_str = str(date)
                date_data = df_all[df_all["BizDt"] == date]

                if date_data.empty:
                    continue

                # Get underlying price
                underlying_price = date_data["UndrlygPric"].dropna()
                if underlying_price.empty:
                    continue
                underlying_price = float(underlying_price.iloc[0])

                # Calculate PCR
                put_volume = date_data[date_data["OptnTp"] == "PE"]["TtlTradgVol"].sum()
                call_volume = date_data[date_data["OptnTp"] == "CE"]["TtlTradgVol"].sum()
                put_oi = date_data[date_data["OptnTp"] == "PE"]["OpnIntrst"].sum()
                call_oi = date_data[date_data["OptnTp"] == "CE"]["OpnIntrst"].sum()

                pcr_volume = round(put_volume / call_volume, 4) if call_volume > 0 else 0
                pcr_oi = round(put_oi / call_oi, 4) if call_oi > 0 else 0

                # Get RSI from cache
                rsi_value = None
                cache_json = date_data["data_json"].dropna()
                if not cache_json.empty:
                    try:
                        cache_data = json.loads(cache_json.iloc[0])
                        ticker_data = next((item for item in cache_data if item.get("stock") == ticker), None)
                        if ticker_data:
                            rsi_value = float(ticker_data.get("rsi")) if ticker_data.get("rsi") else None
                    except:
                        pass

                # Calculate metric value
                data_point = {
                    "date": date_str,
                    "pcr_volume": pcr_volume,
                    "pcr_oi": pcr_oi,
                    "underlying_price": round(underlying_price, 2),
                    "rsi": rsi_value,
                }

                if metric == "money":
                    if option_type == "call":
                        mask = (date_data["OptnTp"] == "CE") & (date_data["StrkPric"] < underlying_price)
                    else:
                        mask = (date_data["OptnTp"] == "PE") & (date_data["StrkPric"] > underlying_price)

                    itm_contracts = date_data[mask]
                    total_money = (itm_contracts["TtlTrfVal"] * abs(itm_contracts["StrkPric"] - underlying_price)).sum()
                    data_point["value"] = float(total_money)
                    data_point["metric_label"] = "Money"

                elif metric == "vega":
                    if strike and strike != "N/A":
                        strike_data = date_data[
                            (date_data["OptnTp"] == opt_type_param) & (date_data["StrkPric"] == float(strike))
                        ]
                        value = float(strike_data["vega"].iloc[0]) if not strike_data.empty else 0
                        data_point["value"] = value
                        data_point["metric_label"] = f"Vega @ {strike}"
                    else:
                        avg_vega = date_data[date_data["OptnTp"] == opt_type_param]["vega"].mean()
                        data_point["value"] = float(avg_vega) if not pd.isna(avg_vega) else 0
                        data_point["metric_label"] = "Avg Vega"

                historical_data.append(data_point)

            except Exception as e:
                print(f"[ERROR] {ticker} | {date} | {e}")
                continue

        return jsonify({"success": True, "ticker": ticker, "data": historical_data})

    except Exception as e:
        print(f"[FATAL] {e}")
        import traceback

        traceback.print_exc()
        return jsonify({"error": str(e), "success": False}), 500
